{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/alechfho/timeseries-lab/blob/main/fbprophet_bike_share.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeseries Lab\n",
    "\n",
    "- Quick tutorial on Jupyter Notebook or Colab\n",
    "- Provide a light weight introduction to Time series\n",
    "- Because this is an introduction, \n",
    "- Introduction to 2 time series models\n",
    "\n",
    "## Time series\n",
    "\n",
    "For typcial ML problems, when we make a prediction about a new observation, that model is built from hundreds or thousands of previous observations that are either all captured at a single point in time, or from data points in which time does not matter. This is known as cross-sectional data.\n",
    "\n",
    "For example, for future house prices in a neighbourhood, we may take a bunch of data points about rooms, renovation state, age etc at different times and train a model to predict future prices. The data may be collected at different time points but the time element and ordering is not important in the model.\n",
    "\n",
    "Time series data is different because it is recorded at regular time intervals, and the order of these data points is important. Therefore, any predictive model based on time series data will have time as an independent variable. The output of a model would be the predicted value or classification at a specific future time **based on a value or values in the past**. \n",
    "\n",
    "Here are a few examples of how different industries use time series forecasting: \n",
    "\n",
    "- Energy – Prices; demand; production schedules\n",
    "- Retail – Sales; consumer demand for certain products\n",
    "- State government – Sales tax receipts\n",
    "- Transportation – Demand for future travel\n",
    "- Finance – Stocks; market potential\n",
    "\n",
    "\n",
    "## Example time series\n",
    "\n",
    "https://www.kaggle.com/chirag19/air-passengers\n",
    "\n",
    "<img src=\"airline_passengers.png\"/>\n",
    "\n",
    "A useful abstraction for selecting forecasting methods is to break a time series down into systematic and unsystematic components.\n",
    "\n",
    "- Systematic: Components of the time series that have consistency or recurrence and can be described and modeled.\n",
    "- Non-Systematic: Components of the time series that cannot be directly modeled.\n",
    "\n",
    "A given time series is thought to consist of three systematic components including level, trend, seasonality, and one non-systematic component called noise.\n",
    "\n",
    "- **Trend**: The increasing or decreasing value in the series.\n",
    "- **Seasonality**: The repeating short-term cycle in the series. \n",
    "- **Level**: The average value in the series.\n",
    "\n",
    "- **Noise (Random)**: The random variation in the series.\n",
    "\n",
    "<img src=\"decomposed_airline_passengers.png\"/>\n",
    "\n",
    "Other important concepts that you hear:\n",
    "\n",
    "- **Stationarity** Time series are stationary if they do not have trend or seasonal effects. Summary statistics calculated on the time series are consistent over time, like the mean or the variance of the observations. When a time series is stationary, it can be easier to model. Statistical modeling methods assume or require the time series to be stationary to be effective\n",
    " \n",
    "\n",
    "\n",
    "We will look at a couple of models for time series forecasting:\n",
    "\n",
    "- **ARIMA**\n",
    "- **Facebook Prophet**\n",
    "\n",
    "They take 2 very different approaches for modeling timeseries\n",
    "\n",
    "### ARIMA model\n",
    "\n",
    "This is a statistical model for analyzing and forecasting time series data. It is composed of 2 timeseries models - Autoregression and Moving Average. Autoregression models the dependent relationship between an observation and some number of lagged observations i.e. the value at time *t+1* has some dependency on the value at time *t*. Moving Average uses a moving average and applies a model to the residual error between the moving average and the actual value observed at time *t*.\n",
    "\n",
    "\n",
    "### FB Prophet model\n",
    "\n",
    "This is a model developed by FaceBook and is based on generalized additive model. The approach uses a curve fitting model plus trends, seasonality and typical events such as public holidays that affect the timeseries. \n",
    "\n",
    "We will go into more detail in the following sections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy pandas pystan fbprophet plotly\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from fbprophet import Prophet\n",
    "from fbprophet.plot import add_changepoints_to_plot\n",
    "\n",
    "%matplotlib inline\n",
    " \n",
    "plt.rcParams['figure.figsize']=(20,10)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 days\n",
    "validation_set_size = 7*24 \n",
    "# 3 days\n",
    "prediction_set_size = 3*24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the data\n",
    "\n",
    "Last year, ML group focused on learning about timeseries forecasting and in the fall of last year, we had an friendly internal competition to see who can use the technique we learnt best. The data we chose to use for the competition was publicly available data from https://bikesharetoronto.com/. \n",
    "\n",
    "## Facts about the data\n",
    "- Every 15 minutes, we collected data from a REST API that list the bike availability by stations throughout Toronto. - There are over 600 stations. Each station is represented by a station id. \n",
    "- The data spans October 2020 and Thanksgiving October 12 last year. This tutorial is based on the bike share data.\n",
    "- The data is aggregated over a month and so each row is denormalized i.e. keyed by last_updated (date) and station_id.\n",
    "\n",
    "## First steps\n",
    "\n",
    "- Read in the bike share data from a parquet file\n",
    "- Set the index to the 'last_updated' and 'station_id' columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_df = pd.read_parquet('https://github.com/alechfho/timeseries-lab/raw/main/data/all_data.parquet.gzip')\n",
    "\n",
    "bike_share_df['last_updated'] = pd.to_datetime(bike_share_df['last_updated'] * 1000, unit='ms')\n",
    "bike_share_df = bike_share_df.set_index(['last_updated', 'station_id'])\n",
    "bike_share_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'Unstack' the index so that everything is keyed by date and the station_id are now columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_stations_df = bike_share_df.unstack()\n",
    "\n",
    "bike_share_stations_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data is at 15 minutes interval and is very fine grain. Let's reduce the timescale to every hour since we will be predicting based on the hour.\n",
    "\n",
    "- Note that here we chose to resample using group by max value. We could have chosen other methods as well.\n",
    "- **Exercise: Look up other time scale**\n",
    "- **Exercise: Look up other groupby methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_share_stations_df_1h = bike_share_stations_df.resample('1h').max()\n",
    "\n",
    "bike_share_stations_df_1h.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with one specific station **Bay and Wellesley - station id 7030**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_7030 = bike_share_stations_df_1h['num_bikes_available']['7030'].to_frame()\n",
    "# can we get rid of df_7030?\n",
    "df = bike_share_stations_df_1h['num_bikes_available']['7030'].to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why FB created Prophet?\n",
    "\n",
    "### What do you notice about the data?\n",
    "\n",
    "<img src=\"events.JPG\"/>\n",
    "\n",
    "Answer:\n",
    "https://github.com/alechfho/timeseries-lab/raw/main/events_annotated.JPG\n",
    "\n",
    "* Forecasting, a common business problem, needed a tool that is more accessible and doesn't require as much expertise\n",
    "* Building classical time series models may be quite involved and is not getting as much love as other ML approaches \n",
    "* Opportunity to build a tool (not all purpose, but) that can be applied in many scenarios like capacity planning (how many users, how fast will the data grow), forecasting number of bugs (translated to staffing) or other like anomaly detection\n",
    "\n",
    "# How\n",
    "* A tool that can cover seasonality, holiday effect, piecewise trends - characteristic to time series describing \"people doing things\"\n",
    "* Building on top of Stan widely used statistical language software package, applied in social science, pharmaceutical statistics, market research\n",
    "\n",
    "# Other characteristics\n",
    "* Decomposable model that can be explained - insights on the level of individual components\n",
    "* Fast - practical in quick iterations when building a model \n",
    "\n",
    "# Prepare for Prophet\n",
    "\n",
    "For prophet to work, we need to change the names of these columns to 'ds' and 'y', so lets just create a new dataframe and keep our old one handy (you'll see why later). The new dataframe will initially be created with an integer index so we can rename the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://facebook.github.io/prophet/docs/quick_start.html\n",
    "\n",
    "The input to Prophet is always a dataframe with two columns: ds and y. The ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp. The y column must be numeric, and represents the measurement we wish to forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "df = df.rename(columns={'last_updated':'ds', '7030':'y'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now's a good time to take a look at your data.  Plot the data using pandas' ```plot``` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('ds').y.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "When working with time-series data, its good to take a look at the data to determine if trends exist, whether it is stationary, has any outliers and/or any other anamolies. Facebook prophet's example uses the log-transform as a way to remove some of these anomolies but it isn't the absolute 'best' way to do this...but given that its the example and a simple data series, I'll follow their lead for now.  Taking the log of a number is easily reversible to be able to see your original data. \n",
    "\n",
    "To log-transform your data, you can use numpy's log() function\n",
    "\n",
    "In this case, we don't see a trend but do see a daily cycle. One strategy is to apply a log transform:\n",
    "\n",
    "    df['y'] = np.log(df['y'])\n",
    "\n",
    "For timeseries data, this is referred to as seasonality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Prophet\n",
    "\n",
    "Now, let's set prophet up to begin modeling our data.\n",
    "\n",
    "Since we are working with daily data, we are turning on daily_seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Prophet(yearly_seasonality=False, daily_seasonality=True, \n",
    "                n_changepoints=10, changepoint_range=0.99, changepoint_prior_scale=0.3)\n",
    "model.add_country_holidays(country_name='CA')\n",
    "model.fit(df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're forecasting 8 hours into the future, we set the ```freq='h'``` and set the period to 8.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = model.make_future_dataframe(periods=8, freq = 'h')\n",
    "future.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Account for holidays\n",
    "\n",
    "https://github.com/dr-prodigy/python-holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_holiday_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To forecast this future data, we need to run it through Prophet's model by calling the ```fit``` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "forecast = model.predict(future)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting forecast dataframe contains quite a bit of data, but we really only care about a few columns.  First, let's look at the full dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_components(forecast);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Prophet results\n",
    "\n",
    "Prophet has a plotting mechanism called ```plot```.  This plot functionality draws the original data (black dots), the model (blue line) and the error of the forecast (shaded blue area)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = model.plot(forecast);\n",
    "a = add_changepoints_to_plot(fig.gca(), model, forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: experiment with the trend using changepoint parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Prophet(yearly_seasonality=False, daily_seasonality=True, \n",
    "                 n_changepoints=10, changepoint_range=0.99, changepoint_prior_scale=0.2)\n",
    "model2.add_country_holidays(country_name='CA')\n",
    "model2.fit(df);\n",
    "forecast2 = model2.predict(model2.make_future_dataframe(periods=8, freq = 'h'))\n",
    "fig2 = model2.plot(forecast2);\n",
    "add_changepoints_to_plot(fig2.gca(), model2, forecast2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section describes how we can build another visualization for Prophet modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Prophet models\n",
    "\n",
    "In order to build a useful dataframe to visualize our model versus our original data, we need to combine the output of the Prophet model with our original data set, then we'll build a new chart manually using pandas and matplotlib.\n",
    "\n",
    "First, let's set our dataframes to have the same index of ```ds```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdf = df.set_index('ds')\n",
    "forecast.set_index('ds', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll combine the original data and our forecast model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_df = vdf.join(forecast[['yhat', 'yhat_lower','yhat_upper']], how = 'outer')\n",
    "\n",
    "viz_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the ```y``` and ```yhat``` data together in a chart. **Note** the last 8 hours in this graph is the predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_df[['y', 'yhat']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with statistical model (ARIMA)\n",
    "ARIMA (Autoregressive Integrated Moving Average Model, or ARIMA for short is a standard statistical model for time series forecast and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages \n",
    "[statsmodels](https://www.statsmodels.org/stable/index.html) is a Python module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration. \n",
    "\n",
    "[sklearn.metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) module includes score functions, performance metrics, pairwise metrics and distance computations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sklearn\n",
    "!pip install --upgrade git+https://github.com/statsmodels/statsmodels\n",
    "    \n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA model and parameters\n",
    "An ARIMA model is a class of statistical models for analyzing and forecasting time series data. ARIMA is an acronym that stands for Auto Regressive Integrated Moving Average. \n",
    "\n",
    "#### Key aspects of the model are:\n",
    "\n",
    "- **AR: *Autoregression.***\n",
    " A model that uses the dependent relationship between an observation and some number of lagged observations. \n",
    "\n",
    "    An autoregression model makes an assumption that the observations at current and previous time steps are useful to predict the value at the next time step. We can use statistical measures to calculate the correlation between the output variable and values at previous time steps at various different lags. \n",
    "    \n",
    "    The stronger the correlation between the output variable and a specified lag variable, the more weight that autoregression model can put on that variable when modeling. Because the correlation is calculated between the variable and itself at previous time steps, it is called an autocorrelation. \n",
    "    \n",
    "    For example, consective values in the Google stock prices appear to follow one another fairly closely:\n",
    "    \n",
    "    <img src=\"autoregression_google_stock.png\" />\n",
    "    <center>Source: https://online.stat.psu.edu/stat501/lesson/14/14.1</center>\n",
    "    \n",
    "    Partial autocorrelation graph for stock prices shows that there is a significant spike at a lag of 1 and much lower spikes for the subsequent lags:\n",
    "    \n",
    "    <img src=\"autocorrelation_google_stock.png\" />\n",
    "    <center>Source: https://online.stat.psu.edu/stat501/lesson/14/14.1</center>\n",
    "    <br>\n",
    "    \n",
    "- **I: *Integrated.***\n",
    "The use of differencing of raw observations (e.g. subtracting an observation from an observation at the previous time step) in order to make the time series stationary.\n",
    "\n",
    "    For example, the plot of antidiabetic drug sales time series shows that it has an increasing trend, strong seasonality, and increasing variation. Transformations such as logarithms can help to stabilize the variance of a time series. Differencing can help stabilize the mean of a time series by removing changes in the level of a time series, and therefore eliminating (or reducing) trend and seasonality. \n",
    "    \n",
    "    <img src=\"differencing_antidiabetic_drugs.png\" width=\"700\"/>\n",
    "    <center>Source: http://course1.winona.edu/bdeppa/FIN%20335/Handouts/ARIMA_Part_1.html</center>\n",
    "\n",
    "    <br>\n",
    "- **MA: *Moving Average.***\n",
    "A model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations. \n",
    "\n",
    "    The difference between what was expected and what was predicted is called residual error. Just like the input observations, residual errors form a time series and can have temporal structure like trends, bias and seasonality. A simple and effective model of residual error is autoregression. Some number of lagged error values are used to predict the error at the next time step. The predict errors are used to correct forecasts. \n",
    "    > *improved forecast = forecast + estimated error*\n",
    "    \n",
    "    An autoregression of the residual error time series is called a Moving Average (MA) model. It has nothing to do with the moving average smoothing process.\n",
    "\n",
    "Each of these components are explicitly specified in the model as a parameter. A standard notation is used of ARIMA(p,d,q) where the parameters are substituted with integer values to quickly indicate the specific ARIMA model being used.\n",
    "\n",
    "#### The parameters of the ARIMA model are defined as follows:\n",
    "\n",
    "- **p**: The number of lag observations included in the model, also called the lag order.\n",
    "- **d**: The number of times that the raw observations are differenced, also called the degree of differencing.\n",
    "- **q**: The size of the moving average window, also called the order of moving average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Grid Search \n",
    "Grid search is the process of training and evaluating models on different combinations of model hyperparameters. In machine learning, a hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are derived via training.\n",
    "\n",
    "ARIMA model for time series forecasting can be tricky to configure. We will evaluate ARIMA model using different combinations of (p, d, q) and find the one with the best score i.e. minimum mean squared error on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hourly data points\n",
    "arima_dataset = df_7030[\"7030\"]\n",
    "arima_train = arima_dataset[1:len(arima_dataset) - prediction_set_size]\n",
    "arima_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function to evaluate arima model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_arima_model(X, arima_order):\n",
    "    # prepare training dataset\n",
    "    train, validation = X[1:len(X) - validation_set_size], X[len(X)-validation_set_size:]\n",
    "    history = [x for x in train]\n",
    "    # make predictions\n",
    "    predictions = list()\n",
    "    for t in range(len(validation)):\n",
    "        model = ARIMA(history, order=arima_order)\n",
    "        model_fit = model.fit()\n",
    "        yhat = model_fit.forecast()[0]\n",
    "        predictions.append(yhat)\n",
    "        history.append(validation[t])\n",
    "    # calculate out-of-sample error\n",
    "    rmse = sqrt(mean_squared_error(validation, predictions))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function to interate over different combinations of parameters and evaluate ARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(dataset, p_values, d_values, q_values):\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "    for p in p_values:\n",
    "        for d in d_values:\n",
    "            for q in q_values:\n",
    "                order = (p,d,q)\n",
    "                try:\n",
    "                    rmse = evaluate_arima_model(dataset, order)\n",
    "                    if rmse < best_score:\n",
    "                        best_score, best_cfg = rmse, order\n",
    "                    print('ARIMA%s RMSE=%.3f' % (order, rmse))\n",
    "                except:\n",
    "                    continue\n",
    "    print('Best ARIMA%s RMSE=%.3f' % (best_cfg, best_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate ARIMA model with different combinations of p, d, and q values\n",
    "We have arbitrarily chosen a small range of values for p, d and q here as an example of grid search. This can take a 5-10 minutes to finish. So be patient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = range(0, 3) # lag observations\n",
    "d_values = range(0, 3) # Number of times raw observations are differenced\n",
    "q_values = range(0, 3) # Size of moving average window\n",
    "\n",
    "evaluate_models(arima_train, p_values, d_values, q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Predictions using best ARIMA parameters\n",
    "Using the best paramters for ARIMA, make predictions 8 hours into the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to make predictions using ARIMA model and given parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(dataset, order, number_of_predictions):\n",
    "    history = [x for x in dataset]\n",
    "    predictions = list()\n",
    "    for t in range(number_of_predictions):\n",
    "        model = ARIMA(history, order=order)\n",
    "        model_fit = model.fit()\n",
    "        output = model_fit.forecast()\n",
    "        yhat = output[0]\n",
    "        predictions.append(yhat)\n",
    "        history.append(yhat)\n",
    "        print('prediction-%d= %f' % (t+1, yhat))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise</b> Make predictions using different values of p,d,q in the ARIMA model e.g. (29, 2, 0), (29, 0, 0), (0, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Update arima_order to different values of p,d,q and make predictions\n",
    "# Also rerun Step 3 code in the next section to replot using the latest predictions\n",
    "arima_order = (29, 2, 0)\n",
    "\n",
    "arima_predictions = make_predictions(arima_train, arima_order, prediction_set_size)\n",
    "arima_predictions_date_range = pd.date_range(arima_train.last_valid_index(), periods=prediction_set_size, freq='H')\n",
    "arima_predictions_df = pd.DataFrame({'last_updated': arima_predictions_date_range, 'yhat_arima':arima_predictions})\n",
    "arima_predictions_df = arima_predictions_df.set_index('last_updated')\n",
    "arima_predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Plot predictions from both models\n",
    "Plot bike share training dataset and predictions generated by ARIMA model and Prophet\n",
    "\n",
    "<b>Exercise</b> Change number of days that are displayed on the graph and visually analyze the model fit over the time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Change number of days to show to 21 days, and then 14 days, and then 7 days\n",
    "number_of_days_to_show = 28\n",
    "comparison_df = arima_predictions_df.join(viz_df[['y', 'yhat']], how = 'outer').tail(24*number_of_days_to_show)\n",
    "comparison_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of ARIMA predictions (Zoomed in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df_predictions_only = arima_predictions_df.join(viz_df[['y', 'yhat']], how = 'outer').tail(prediction_set_size)\n",
    "comparison_df_predictions_only.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Introduction to Time Series Forecasting with Python - Jason Brownlee](https://machinelearningmastery.com/introduction-to-time-series-forecasting-with-python/)\n",
    "- [STAT501 - Penn State](https://online.stat.psu.edu/stat501)\n",
    "- [ARIMA Models - Rob Hyndman](http://course1.winona.edu/bdeppa/FIN%20335/Handouts/ARIMA_Part_1.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
